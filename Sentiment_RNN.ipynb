{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/reviews.txt','r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt','r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:1000])\n",
    "print()\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把所有文字都用array存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "reviews = reviews.lower()\n",
    "\n",
    "words = ''.join([c for c in reviews if c not in punctuation])\n",
    "# words[:100]\n",
    "\n",
    "review_split = words.split('\\n')\n",
    "words = ' '.join(review_split)\n",
    "words = words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將每則評論(review)encode到數字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> padding用0 , 所以vocab to int要從1開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(words)\n",
    "\n",
    "# vovab_to_int\n",
    "counts = sorted(counts,key=counts.get,reverse=True)\n",
    "vocab_to_int = { w:i for i,w in enumerate(counts,1) }\n",
    "\n",
    "review_to_int = []\n",
    "for review in review_split:\n",
    "    \n",
    "    review_to_int.append([vocab_to_int[w] for w in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21025,\n",
       "  308,\n",
       "  6,\n",
       "  3,\n",
       "  1050,\n",
       "  207,\n",
       "  8,\n",
       "  2138,\n",
       "  32,\n",
       "  1,\n",
       "  171,\n",
       "  57,\n",
       "  15,\n",
       "  49,\n",
       "  81,\n",
       "  5785,\n",
       "  44,\n",
       "  382,\n",
       "  110,\n",
       "  140,\n",
       "  15,\n",
       "  5194,\n",
       "  60,\n",
       "  154,\n",
       "  9,\n",
       "  1,\n",
       "  4975,\n",
       "  5852,\n",
       "  475,\n",
       "  71,\n",
       "  5,\n",
       "  260,\n",
       "  12,\n",
       "  21025,\n",
       "  308,\n",
       "  13,\n",
       "  1978,\n",
       "  6,\n",
       "  74,\n",
       "  2395,\n",
       "  5,\n",
       "  613,\n",
       "  73,\n",
       "  6,\n",
       "  5194,\n",
       "  1,\n",
       "  24103,\n",
       "  5,\n",
       "  1983,\n",
       "  10166,\n",
       "  1,\n",
       "  5786,\n",
       "  1499,\n",
       "  36,\n",
       "  51,\n",
       "  66,\n",
       "  204,\n",
       "  145,\n",
       "  67,\n",
       "  1199,\n",
       "  5194,\n",
       "  19869,\n",
       "  1,\n",
       "  37442,\n",
       "  4,\n",
       "  1,\n",
       "  221,\n",
       "  883,\n",
       "  31,\n",
       "  2988,\n",
       "  71,\n",
       "  4,\n",
       "  1,\n",
       "  5787,\n",
       "  10,\n",
       "  686,\n",
       "  2,\n",
       "  67,\n",
       "  1499,\n",
       "  54,\n",
       "  10,\n",
       "  216,\n",
       "  1,\n",
       "  383,\n",
       "  9,\n",
       "  62,\n",
       "  3,\n",
       "  1406,\n",
       "  3686,\n",
       "  783,\n",
       "  5,\n",
       "  3483,\n",
       "  180,\n",
       "  1,\n",
       "  382,\n",
       "  10,\n",
       "  1212,\n",
       "  13583,\n",
       "  32,\n",
       "  308,\n",
       "  3,\n",
       "  349,\n",
       "  341,\n",
       "  2913,\n",
       "  10,\n",
       "  143,\n",
       "  127,\n",
       "  5,\n",
       "  7690,\n",
       "  30,\n",
       "  4,\n",
       "  129,\n",
       "  5194,\n",
       "  1406,\n",
       "  2326,\n",
       "  5,\n",
       "  21025,\n",
       "  308,\n",
       "  10,\n",
       "  528,\n",
       "  12,\n",
       "  109,\n",
       "  1448,\n",
       "  4,\n",
       "  60,\n",
       "  543,\n",
       "  102,\n",
       "  12,\n",
       "  21025,\n",
       "  308,\n",
       "  6,\n",
       "  227,\n",
       "  4146,\n",
       "  48,\n",
       "  3,\n",
       "  2211,\n",
       "  12,\n",
       "  8,\n",
       "  215,\n",
       "  23]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_int[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode labels (1:positive  ,  0:negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = [1 if label=='positive' else 0 for label in labels.split(\"\\n\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 刪一下離群值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 刪一下長度為0的review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(review_to_int))\n",
    "\n",
    "none_zero_idx = [ idx for idx,review in enumerate(review_to_int) if len(review)!=0]\n",
    "none_zero_review = [review_to_int[idx] for idx in none_zero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(none_zero_review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = [ v for i,v in enumerate(encoded_labels) if i in none_zero_idx]\n",
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 限制一個長度, 超過clip掉,太少補0在前面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_to_int = np.array(none_zero_review[:])\n",
    "encoded_labels = np.array(encoded_labels[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = encoded_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(review_int , seq_len):\n",
    "    \n",
    "    features = np.zeros((len(review_int),seq_len),dtype=np.int)\n",
    "    for i,r in enumerate(review_int):\n",
    "        features[i,-len(r):] =  np.array(r)[:seq_len]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]\n",
      " [   54    10    84   329 26230 46427    63    10    14   614]\n",
      " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   40    26   109 17952  1422     9     1   327     4   125]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   10   499     1   307 10399    55    74     8    13    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "features = padding_seq(review_to_int , seq_len)\n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 80% train , 10% valid , 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_ratio = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x , remain_x = np.array(features[:split_idx]) , np.array(features[split_idx:])\n",
    "train_y ,remain_y = np.array(encoded_labels[:split_idx]) , np.array(encoded_labels[split_idx:])\n",
    "\n",
    "frac_ratio = len(remain_x)//2\n",
    "valid_x,test_x = remain_x[:frac_ratio],remain_x[frac_ratio:]\n",
    "valid_y,test_y = remain_y[:frac_ratio],remain_y[frac_ratio:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset , DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset( torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset( torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset( torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data ,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader(valid_data ,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_data ,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      0,      0,  ...,   6742,     45,      4],\n",
      "        [    10,     84,    548,  ...,     53,      6,    894],\n",
      "        [     0,      0,      0,  ...,    147,    287,    291],\n",
      "        ...,\n",
      "        [  1540,   1963,  25099,  ...,   1729,      3,  22398],\n",
      "        [     0,      0,      0,  ...,   1918,     16,    336],\n",
      "        [     0,      0,      0,  ...,      2,    440,     20]])\n",
      "tensor([ 0,  1,  0,  1,  1,  1,  1,  0,  1,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  1,  1,  1,  0,  1,  0,  1,  0,  1,\n",
      "         0,  0,  1,  1,  1,  0,  0,  0,  1,  1,  0,  0,  1,  1,\n",
      "         0,  1,  0,  0,  1,  0,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "sample_x,sample_y = next(iter(train_loader))\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train which device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on GPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print('train on GPU')\n",
    "else:\n",
    "    print('train on cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self , vocab_size, output_size , embedding_dim , hidden_size , n_layer=2 , drop_prob=0.5):\n",
    "        \n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.embed_dim = embedding_dim\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_size,n_layer,dropout=drop_prob,batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        r_out,hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        out = r_out.contiguous().view(-1,self.hidden_dim)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        \n",
    "        out = out.view(batch_size,-1)\n",
    "        out = out[:,-1]\n",
    "        \n",
    "        return out,hidden\n",
    "        \n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        # 新增兩個tensor 給 hidden state 和 cell state\n",
    "        # 兩個tensor的shape = n_layer * batch_size * hidden_dim  (順序是固定,不能變動)\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layer,batch_size,self.hidden_dim).zero_().cuda() , \n",
    "                     weight.new(self.n_layer,batch_size,self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layer,batch_size,self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layer,batch_size,self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1  # +1 for padding\n",
    "output_size = 1   # 只輸出pos or neg\n",
    "embed_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layer = 2\n",
    "\n",
    "\n",
    "net = RNN(vocab_size , output_size , embed_dim , hidden_dim , n_layer)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練囉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 4\n",
    "print_every = 100\n",
    "\n",
    "clip = 5\n",
    "counter = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "opt = torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 step: 500 Loss: 0.4309 valid_Loss 0.5483\n",
      "Epoch: 1/4 step: 600 Loss: 0.3964 valid_Loss 0.4810\n",
      "Epoch: 1/4 step: 700 Loss: 0.3857 valid_Loss 0.4888\n",
      "Epoch: 1/4 step: 800 Loss: 0.2996 valid_Loss 0.4762\n",
      "Epoch: 2/4 step: 900 Loss: 0.3196 valid_Loss 0.4985\n",
      "Epoch: 2/4 step: 1000 Loss: 0.4524 valid_Loss 0.5489\n",
      "Epoch: 2/4 step: 1100 Loss: 0.3328 valid_Loss 0.4814\n",
      "Epoch: 2/4 step: 1200 Loss: 0.2761 valid_Loss 0.4586\n",
      "Epoch: 3/4 step: 1300 Loss: 0.1016 valid_Loss 0.5076\n",
      "Epoch: 3/4 step: 1400 Loss: 0.1097 valid_Loss 0.5038\n",
      "Epoch: 3/4 step: 1500 Loss: 0.1801 valid_Loss 0.6004\n",
      "Epoch: 3/4 step: 1600 Loss: 0.1585 valid_Loss 0.5138\n",
      "Epoch: 4/4 step: 1700 Loss: 0.1140 valid_Loss 0.5865\n",
      "Epoch: 4/4 step: 1800 Loss: 0.0709 valid_Loss 0.5724\n",
      "Epoch: 4/4 step: 1900 Loss: 0.0412 valid_Loss 0.5717\n",
      "Epoch: 4/4 step: 2000 Loss: 0.0966 valid_Loss 0.5821\n"
     ]
    }
   ],
   "source": [
    "if train_on_gpu:\n",
    "    net = net.cuda()\n",
    "net.train()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for idx , (inputs , labels) in enumerate(train_loader):\n",
    "        counter += 1\n",
    "        if train_on_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            \n",
    "        # 每次batch 都要create新的variables for hidden state, 不然會一直累積\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        output , h = net(inputs , h)\n",
    "        loss = criterion(output.squeeze() , labels.float())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        ## 要clip gradient ,不然會梯度爆炸\n",
    "        nn.utils.clip_grad_norm_(net.parameters() , clip) \n",
    "        opt.step()\n",
    "\n",
    "        if counter % print_every ==0:\n",
    "\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "\n",
    "            for val_inputs , val_labels in valid_loader:\n",
    "\n",
    "                if train_on_gpu:\n",
    "                    val_inputs,val_labels = val_inputs.cuda() , val_labels.cuda()\n",
    "                val_h = tuple([ each.data for each in val_h ])\n",
    "\n",
    "                val_out , val_h = net(val_inputs,val_h)\n",
    "                val_loss = criterion(val_out.squeeze() , val_labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}\".format(epoch+1,epochs),\n",
    "                 \"step: {}\".format(counter),\n",
    "                 \"Loss: {:.4f}\".format(loss.item()),\n",
    "                 \"valid_Loss {:.4f}\".format(np.mean(val_losses)))\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 跑test data囉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.5589\n",
      "test acc: 0.8032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "test_losses=[]\n",
    "num_correct=0\n",
    "net.eval()\n",
    "\n",
    "\n",
    "for inputs,labels in test_loader:\n",
    "    \n",
    "    \n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    if train_on_gpu:\n",
    "        inputs,labels = inputs.cuda(),labels.cuda()\n",
    "    out , h = net(inputs,h)\n",
    "    \n",
    "    test_loss = criterion(out.squeeze(),labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(out.squeeze())\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"test loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"test acc: {:.4f}\".format(num_correct/len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 來試試看打一些評論 看看model預測的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_pos = \"The movie is very good. I like this movie, it's so DOPE !!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize(review):\n",
    "    review = review.lower()\n",
    "    text = ''.join([c for c in review if c not in punctuation])\n",
    "    \n",
    "    text_arr = text.split()\n",
    "    text_int = [ vocab_to_int[w] for w in text_arr]\n",
    "    return [text_int]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 18, 6, 55, 50, 10, 39, 11, 18, 92, 37, 10417]]\n"
     ]
    }
   ],
   "source": [
    "test_int = tokenize(review_pos)\n",
    "print(test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     1    18     6    55\n",
      "     50    10    39    11    18    92    37 10417]]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "pad_test = padding_seq(test_int,seq_len)\n",
    "print(pad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net,test_review,seq_len=200):\n",
    "    net.eval()\n",
    "    test_int = tokenize(test_review)\n",
    "    pad_test = padding_seq(test_int,seq_len)\n",
    "    \n",
    "    \n",
    "    review_tensor = torch.from_numpy(pad_test)\n",
    "    batch_size = review_tensor.size(0)\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        review_tensor = review_tensor.cuda()\n",
    "    out , h = net(review_tensor,h)\n",
    "    \n",
    "    pred = torch.round(out.squeeze())\n",
    "    \n",
    "    if pred.item()==1:\n",
    "        print(\"review is positive !\")\n",
    "    else:\n",
    "        print(\"review is negative !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review is positive !\n"
     ]
    }
   ],
   "source": [
    "predict(net,review_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_neg = \"Damn, this movie is like shit. I don't see it any more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review is negative !\n"
     ]
    }
   ],
   "source": [
    "predict(net,review_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
